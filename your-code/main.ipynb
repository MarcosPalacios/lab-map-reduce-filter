{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Before-your-start:\" data-toc-modified-id=\"Before-your-start:-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Before your start:</a></span></li><li><span><a href=\"#Challenge-1---Mapping\" data-toc-modified-id=\"Challenge-1---Mapping-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Challenge 1 - Mapping</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#We-will-use-the-map-function-to-clean-up-a-words-in-a-book.\" data-toc-modified-id=\"We-will-use-the-map-function-to-clean-up-a-words-in-a-book.-2.0.0.1\"><span class=\"toc-item-num\">2.0.0.1&nbsp;&nbsp;</span>We will use the map function to clean up a words in a book.</a></span></li><li><span><a href=\"#Let's-remove-the-first-568-words-since-they-contain-information-about-the-book-but-are-not-part-of-the-book-itself.\" data-toc-modified-id=\"Let's-remove-the-first-568-words-since-they-contain-information-about-the-book-but-are-not-part-of-the-book-itself.-2.0.0.2\"><span class=\"toc-item-num\">2.0.0.2&nbsp;&nbsp;</span>Let's remove the first 568 words since they contain information about the book but are not part of the book itself.</a></span></li><li><span><a href=\"#The-next-step-is-to-create-a-function-that-will-remove-references.\" data-toc-modified-id=\"The-next-step-is-to-create-a-function-that-will-remove-references.-2.0.0.3\"><span class=\"toc-item-num\">2.0.0.3&nbsp;&nbsp;</span>The next step is to create a function that will remove references.</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge-2---Filtering\" data-toc-modified-id=\"Challenge-2---Filtering-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Challenge 2 - Filtering</a></span></li><li><span><a href=\"#Bonus-Challenge---Part-1\" data-toc-modified-id=\"Bonus-Challenge---Part-1-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Bonus Challenge - Part 1</a></span></li><li><span><a href=\"#Challenge-3---Reducing\" data-toc-modified-id=\"Challenge-3---Reducing-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Challenge 3 - Reducing</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Now-that-we-have-significantly-cleaned-up-our-text-corpus,-let's-use-the-reduce()-function-to-put-the-words-back-together-into-one-long-string-separated-by-spaces.\" data-toc-modified-id=\"Now-that-we-have-significantly-cleaned-up-our-text-corpus,-let's-use-the-reduce()-function-to-put-the-words-back-together-into-one-long-string-separated-by-spaces.-5.0.0.1\"><span class=\"toc-item-num\">5.0.0.1&nbsp;&nbsp;</span>Now that we have significantly cleaned up our text corpus, let's use the <code>reduce()</code> function to put the words back together into one long string separated by spaces.</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Challenge-4---Applying-Functions-to-DataFrames\" data-toc-modified-id=\"Challenge-4---Applying-Functions-to-DataFrames-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Challenge 4 - Applying Functions to DataFrames</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Our-next-step-is-to-use-the-apply-function-to-a-dataframe-and-transform-all-cells.\" data-toc-modified-id=\"Our-next-step-is-to-use-the-apply-function-to-a-dataframe-and-transform-all-cells.-6.0.0.1\"><span class=\"toc-item-num\">6.0.0.1&nbsp;&nbsp;</span>Our next step is to use the apply function to a dataframe and transform all cells.</a></span></li><li><span><a href=\"#Our-last-challenge-will-be-to-create-an-aggregate-function-and-apply-it-to-a-select-group-of-columns-in-our-dataframe.\" data-toc-modified-id=\"Our-last-challenge-will-be-to-create-an-aggregate-function-and-apply-it-to-a-select-group-of-columns-in-our-dataframe.-6.0.0.2\"><span class=\"toc-item-num\">6.0.0.2&nbsp;&nbsp;</span>Our last challenge will be to create an aggregate function and apply it to a select group of columns in our dataframe.</a></span></li></ul></li></ul></li></ul></li><li><span><a href=\"#Bonus-Challenge---Part-2\" data-toc-modified-id=\"Bonus-Challenge---Part-2-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Bonus Challenge - Part 2</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before your start:\n",
    "- Read the README.md file\n",
    "- Comment as much as you can and use the resources in the README.md file\n",
    "- Happy learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import reduce from functools, numpy and pandas\n",
    "from functools import reduce\n",
    "import numpy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1 - Mapping\n",
    "\n",
    "#### We will use the map function to clean up a words in a book.\n",
    "\n",
    "In the following cell, we will read a text file containing the book The Prophet by Khalil Gibran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code:\n",
    "\n",
    "location = '../58585-0.txt'\n",
    "with open(location, 'r', encoding=\"utf8\") as f:\n",
    "    prophet = f.read().split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's remove the first 568 words since they contain information about the book but are not part of the book itself. \n",
    "\n",
    "Do this by removing from `prophet` elements 0 through 567 of the list (you can also do this by keeping elements 568 through the last element)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "prophet = prophet[568:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look through the words, you will find that many words have a reference attached to them. For example, let's look at words 1 through 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dispense',\n",
       " 'with',\n",
       " 'confidence?\\n\\nIf',\n",
       " 'this',\n",
       " 'is',\n",
       " 'my',\n",
       " 'day',\n",
       " 'of',\n",
       " 'harvest,',\n",
       " 'in',\n",
       " 'what\\nfields',\n",
       " 'have',\n",
       " 'I',\n",
       " 'sowed',\n",
       " 'the',\n",
       " 'seed,',\n",
       " 'and',\n",
       " 'in\\nwhat',\n",
       " 'unremembered',\n",
       " 'seasons?\\n\\nIf',\n",
       " 'this',\n",
       " 'indeed',\n",
       " 'be',\n",
       " 'the',\n",
       " 'hour',\n",
       " 'in',\n",
       " 'which',\n",
       " 'I\\nlift',\n",
       " 'up',\n",
       " 'my',\n",
       " 'lantern,',\n",
       " 'it',\n",
       " 'is',\n",
       " 'not',\n",
       " 'my',\n",
       " 'flame\\nthat',\n",
       " 'shall',\n",
       " 'burn',\n",
       " 'therein.\\n\\nEmpty',\n",
       " 'and',\n",
       " 'dark',\n",
       " 'shall',\n",
       " 'I',\n",
       " 'raise',\n",
       " 'my',\n",
       " 'lantern,\\n\\nAnd',\n",
       " 'the',\n",
       " 'guardian',\n",
       " 'of',\n",
       " 'the',\n",
       " 'night',\n",
       " 'shall',\n",
       " 'fill\\nit',\n",
       " 'with',\n",
       " 'oil',\n",
       " 'and',\n",
       " 'he',\n",
       " 'shall',\n",
       " 'light',\n",
       " 'it',\n",
       " 'also.\\n\\n*****\\n\\nThese',\n",
       " 'things',\n",
       " 'he',\n",
       " 'said',\n",
       " 'in',\n",
       " 'words.',\n",
       " 'But',\n",
       " 'much\\nin',\n",
       " 'his',\n",
       " 'heart',\n",
       " 'remained',\n",
       " 'unsaid.',\n",
       " 'For',\n",
       " '{12}he\\nhimself',\n",
       " 'could',\n",
       " 'not',\n",
       " 'speak',\n",
       " 'his',\n",
       " 'deeper\\nsecret.\\n\\n*****\\n\\n[Illustration:',\n",
       " '0020]\\n\\nAnd',\n",
       " 'when',\n",
       " 'he',\n",
       " 'entered',\n",
       " 'into',\n",
       " 'the',\n",
       " 'city',\n",
       " 'all\\nthe',\n",
       " 'people',\n",
       " 'came',\n",
       " 'to',\n",
       " 'meet',\n",
       " 'him,',\n",
       " 'and',\n",
       " 'they\\nwere',\n",
       " 'crying',\n",
       " 'out',\n",
       " 'to',\n",
       " 'him',\n",
       " 'as',\n",
       " 'with',\n",
       " 'one\\nvoice.\\n\\nAnd',\n",
       " 'the',\n",
       " 'elders',\n",
       " 'of',\n",
       " 'the',\n",
       " 'city',\n",
       " 'stood',\n",
       " 'forth\\nand',\n",
       " 'said:\\n\\nGo',\n",
       " 'not',\n",
       " 'yet',\n",
       " 'away',\n",
       " 'from',\n",
       " 'us.\\n\\nA',\n",
       " 'noontide',\n",
       " 'have',\n",
       " 'you',\n",
       " 'been',\n",
       " 'in',\n",
       " 'our\\ntwilight,',\n",
       " 'and',\n",
       " 'your',\n",
       " 'youth',\n",
       " 'has',\n",
       " 'given',\n",
       " 'us\\ndreams',\n",
       " 'to',\n",
       " 'dream.\\n\\nNo',\n",
       " 'stranger',\n",
       " 'are',\n",
       " 'you',\n",
       " 'among',\n",
       " 'us,',\n",
       " 'nor\\na',\n",
       " 'guest,',\n",
       " 'but',\n",
       " 'our',\n",
       " 'son',\n",
       " 'and',\n",
       " 'our',\n",
       " 'dearly\\nbeloved.\\n\\nSuffer',\n",
       " 'not',\n",
       " 'yet',\n",
       " 'our',\n",
       " 'eyes',\n",
       " 'to',\n",
       " 'hunger',\n",
       " 'for\\nyour',\n",
       " 'face.\\n\\n*****\\n\\nAnd',\n",
       " 'the',\n",
       " 'priests',\n",
       " 'and',\n",
       " 'the',\n",
       " 'priestesses',\n",
       " 'said\\nunto',\n",
       " 'him:\\n\\nLet',\n",
       " 'not',\n",
       " 'the',\n",
       " 'waves',\n",
       " 'of',\n",
       " 'the',\n",
       " 'sea',\n",
       " 'separate',\n",
       " 'us\\nnow,',\n",
       " 'and',\n",
       " 'the',\n",
       " 'years',\n",
       " 'you',\n",
       " 'have',\n",
       " 'spent',\n",
       " 'in',\n",
       " 'our\\nmidst',\n",
       " 'become',\n",
       " 'a',\n",
       " 'memory.\\n\\nYou',\n",
       " 'have',\n",
       " 'walked',\n",
       " 'among',\n",
       " 'us',\n",
       " 'a',\n",
       " 'spirit,\\n{13}and',\n",
       " 'your',\n",
       " 'shadow',\n",
       " 'has',\n",
       " 'been',\n",
       " 'a',\n",
       " 'light\\nupon',\n",
       " 'our',\n",
       " 'faces.\\n\\nMuch',\n",
       " 'have',\n",
       " 'we',\n",
       " 'loved',\n",
       " 'you.',\n",
       " 'But',\n",
       " 'speechless\\nwas',\n",
       " 'our',\n",
       " 'love,',\n",
       " 'and',\n",
       " 'with',\n",
       " 'veils',\n",
       " 'has',\n",
       " 'it',\n",
       " 'been\\nveiled.\\n\\nYet',\n",
       " 'now',\n",
       " 'it',\n",
       " 'cries',\n",
       " 'aloud',\n",
       " 'unto',\n",
       " 'you,',\n",
       " 'and\\nwould',\n",
       " 'stand',\n",
       " 'revealed',\n",
       " 'before',\n",
       " 'you.\\n\\nAnd',\n",
       " 'ever',\n",
       " 'has',\n",
       " 'it',\n",
       " 'been',\n",
       " 'that',\n",
       " 'love',\n",
       " 'knows\\nnot',\n",
       " 'its',\n",
       " 'own',\n",
       " 'depth',\n",
       " 'until',\n",
       " 'the',\n",
       " 'hour',\n",
       " 'of\\nseparation.\\n\\n*****\\n\\nAnd',\n",
       " 'others',\n",
       " 'came',\n",
       " 'also',\n",
       " 'and',\n",
       " 'entreated',\n",
       " 'him.\\nBut',\n",
       " 'he',\n",
       " 'answered',\n",
       " 'them',\n",
       " 'not.',\n",
       " 'He',\n",
       " 'only',\n",
       " 'bent\\nhis',\n",
       " 'head;',\n",
       " 'and',\n",
       " 'those',\n",
       " 'who',\n",
       " 'stood',\n",
       " 'near',\n",
       " 'saw\\nhis',\n",
       " 'tears',\n",
       " 'falling',\n",
       " 'upon',\n",
       " 'his',\n",
       " 'breast.\\n\\nAnd',\n",
       " 'he',\n",
       " 'and',\n",
       " 'the',\n",
       " 'people',\n",
       " 'proceeded',\n",
       " 'towards\\nthe',\n",
       " 'great',\n",
       " 'square',\n",
       " 'before',\n",
       " 'the',\n",
       " 'temple.\\n\\nAnd',\n",
       " 'there',\n",
       " 'came',\n",
       " 'out',\n",
       " 'of',\n",
       " 'the',\n",
       " 'sanctuary',\n",
       " 'a\\nwoman',\n",
       " 'whose',\n",
       " 'name',\n",
       " 'was',\n",
       " 'Almitra.',\n",
       " 'And',\n",
       " 'she\\nwas',\n",
       " 'a',\n",
       " 'seeress.\\n\\nAnd',\n",
       " 'he',\n",
       " 'looked',\n",
       " 'upon',\n",
       " 'her',\n",
       " 'with',\n",
       " 'exceeding\\ntenderness,',\n",
       " 'for',\n",
       " 'it',\n",
       " 'was',\n",
       " 'she',\n",
       " 'who',\n",
       " 'had',\n",
       " 'first\\nsought',\n",
       " 'and',\n",
       " 'believed',\n",
       " 'in',\n",
       " 'him',\n",
       " 'when',\n",
       " 'he',\n",
       " 'had\\nbeen',\n",
       " 'but',\n",
       " 'a',\n",
       " 'day',\n",
       " 'in',\n",
       " 'their',\n",
       " 'city.',\n",
       " '{14}And\\nshe',\n",
       " 'hailed',\n",
       " 'him,',\n",
       " 'saying:\\n\\nProphet',\n",
       " 'of',\n",
       " 'God,',\n",
       " 'in',\n",
       " 'quest',\n",
       " 'of',\n",
       " 'the\\nuttermost,',\n",
       " 'long',\n",
       " 'have',\n",
       " 'you',\n",
       " 'searched',\n",
       " 'the\\ndistances',\n",
       " 'for',\n",
       " 'your',\n",
       " 'ship.\\n\\nAnd',\n",
       " 'now',\n",
       " 'your',\n",
       " 'ship',\n",
       " 'has',\n",
       " 'come,',\n",
       " 'and',\n",
       " 'you',\n",
       " 'must\\nneeds',\n",
       " 'go.\\n\\nDeep',\n",
       " 'is',\n",
       " 'your',\n",
       " 'longing',\n",
       " 'for',\n",
       " 'the',\n",
       " 'land',\n",
       " 'of\\nyour',\n",
       " 'memories',\n",
       " 'and',\n",
       " 'the',\n",
       " 'dwelling',\n",
       " 'place\\nof',\n",
       " 'your',\n",
       " 'greater',\n",
       " 'desires;',\n",
       " 'and',\n",
       " 'our',\n",
       " 'love\\nwould',\n",
       " 'not',\n",
       " 'bind',\n",
       " 'you',\n",
       " 'nor',\n",
       " 'our',\n",
       " 'needs',\n",
       " 'hold\\nyou.\\n\\nYet',\n",
       " 'this',\n",
       " 'we',\n",
       " 'ask',\n",
       " 'ere',\n",
       " 'you',\n",
       " 'leave',\n",
       " 'us,',\n",
       " 'that\\nyou',\n",
       " 'speak',\n",
       " 'to',\n",
       " 'us',\n",
       " 'and',\n",
       " 'give',\n",
       " 'us',\n",
       " 'of',\n",
       " 'your\\ntruth.\\n\\nAnd',\n",
       " 'we',\n",
       " 'will',\n",
       " 'give',\n",
       " 'it',\n",
       " 'unto',\n",
       " 'our',\n",
       " 'children,\\nand',\n",
       " 'they',\n",
       " 'unto',\n",
       " 'their',\n",
       " 'children,',\n",
       " 'and',\n",
       " 'it\\nshall',\n",
       " 'not',\n",
       " 'perish.\\n\\nIn',\n",
       " 'your',\n",
       " 'aloneness',\n",
       " 'you',\n",
       " 'have',\n",
       " 'watched',\n",
       " 'with\\nour',\n",
       " 'days,',\n",
       " 'and',\n",
       " 'in',\n",
       " 'your',\n",
       " 'wakefulness',\n",
       " 'you\\nhave',\n",
       " 'listened',\n",
       " 'to',\n",
       " 'the',\n",
       " 'weeping',\n",
       " 'and',\n",
       " 'the\\nlaughter',\n",
       " 'of',\n",
       " 'our',\n",
       " 'sleep.\\n\\nNow',\n",
       " 'therefore',\n",
       " 'disclose',\n",
       " 'us',\n",
       " 'to',\n",
       " 'ourselves,\\nand',\n",
       " 'tell',\n",
       " 'us',\n",
       " 'all',\n",
       " 'that',\n",
       " 'has',\n",
       " 'been',\n",
       " 'shown\\nyou',\n",
       " 'of',\n",
       " 'that',\n",
       " 'which',\n",
       " 'is',\n",
       " 'between',\n",
       " 'birth',\n",
       " 'and\\ndeath.\\n\\n*****\\n\\nAnd',\n",
       " 'he',\n",
       " 'answered,\\n\\nPeople',\n",
       " 'of',\n",
       " 'Orphalese,',\n",
       " 'of',\n",
       " 'what',\n",
       " 'can',\n",
       " 'I\\n{15}speak',\n",
       " 'save',\n",
       " 'of',\n",
       " 'that',\n",
       " 'which',\n",
       " 'is',\n",
       " 'even',\n",
       " 'now\\nmoving',\n",
       " 'within',\n",
       " 'your',\n",
       " 'souls?\\n\\n*****',\n",
       " '*****\\n\\nThen',\n",
       " 'said',\n",
       " 'Almitra,',\n",
       " 'Speak',\n",
       " 'to',\n",
       " 'us',\n",
       " 'of\\n_Love_.\\n\\nAnd',\n",
       " 'he',\n",
       " 'raised',\n",
       " 'his',\n",
       " 'head',\n",
       " 'and',\n",
       " 'looked',\n",
       " 'upon\\nthe',\n",
       " 'people,',\n",
       " 'and',\n",
       " 'there',\n",
       " 'fell',\n",
       " 'a',\n",
       " 'stillness\\nupon',\n",
       " 'them.',\n",
       " 'And',\n",
       " 'with',\n",
       " 'a',\n",
       " 'great',\n",
       " 'voice',\n",
       " 'he\\nsaid:\\n\\nWhen',\n",
       " 'love',\n",
       " 'beckons',\n",
       " 'to',\n",
       " 'you,',\n",
       " 'follow',\n",
       " 'him,\\n\\nThough',\n",
       " 'his',\n",
       " 'ways',\n",
       " 'are',\n",
       " 'hard',\n",
       " 'and',\n",
       " 'steep.\\n\\nAnd',\n",
       " 'when',\n",
       " 'his',\n",
       " 'wings',\n",
       " 'enfold',\n",
       " 'you',\n",
       " 'yield',\n",
       " 'to\\nhim,\\n\\nThough',\n",
       " 'the',\n",
       " 'sword',\n",
       " 'hidden',\n",
       " 'among',\n",
       " 'his\\npinions',\n",
       " 'may',\n",
       " 'wound',\n",
       " 'you.\\n\\nAnd',\n",
       " 'when',\n",
       " 'he',\n",
       " 'speaks',\n",
       " 'to',\n",
       " 'you',\n",
       " 'believe',\n",
       " 'in\\nhim,\\n\\nThough',\n",
       " 'his',\n",
       " 'voice',\n",
       " 'may',\n",
       " 'shatter',\n",
       " 'your',\n",
       " 'dreams\\nas',\n",
       " 'the',\n",
       " 'north',\n",
       " 'wind',\n",
       " 'lays',\n",
       " 'waste',\n",
       " 'the',\n",
       " 'garden.\\n\\nFor',\n",
       " 'even',\n",
       " 'as',\n",
       " 'love',\n",
       " 'crowns',\n",
       " 'you',\n",
       " 'so',\n",
       " 'shall\\nhe',\n",
       " 'crucify',\n",
       " 'you.',\n",
       " 'Even',\n",
       " 'as',\n",
       " 'he',\n",
       " 'is',\n",
       " 'for',\n",
       " 'your\\ngrowth',\n",
       " 'so',\n",
       " 'is',\n",
       " 'he',\n",
       " 'for',\n",
       " 'your',\n",
       " 'pruning.\\n\\nEven',\n",
       " 'as',\n",
       " 'he',\n",
       " 'ascends',\n",
       " 'to',\n",
       " 'your',\n",
       " 'height',\n",
       " 'and\\n{16}caresses',\n",
       " 'your',\n",
       " 'tenderest',\n",
       " 'branches\\nthat',\n",
       " 'quiver',\n",
       " 'in',\n",
       " 'the',\n",
       " 'sun,\\n\\nSo',\n",
       " 'shall',\n",
       " 'he',\n",
       " 'descend',\n",
       " 'to',\n",
       " 'your',\n",
       " 'roots',\n",
       " 'and\\nshake',\n",
       " 'them',\n",
       " 'in',\n",
       " 'their',\n",
       " 'clinging',\n",
       " 'to',\n",
       " 'the\\nearth.\\n\\n*****\\n\\nLike',\n",
       " 'sheaves',\n",
       " 'of',\n",
       " 'corn',\n",
       " 'he',\n",
       " 'gathers',\n",
       " 'you',\n",
       " 'unto\\nhimself.\\n\\nHe',\n",
       " 'threshes',\n",
       " 'you',\n",
       " 'to',\n",
       " 'make',\n",
       " 'you',\n",
       " 'naked.\\n\\nHe',\n",
       " 'sifts',\n",
       " 'you',\n",
       " 'to',\n",
       " 'free',\n",
       " 'you',\n",
       " 'from',\n",
       " 'your\\nhusks.\\n\\nHe',\n",
       " 'grinds',\n",
       " 'you',\n",
       " 'to',\n",
       " 'whiteness.\\n\\nHe',\n",
       " 'kneads',\n",
       " 'you',\n",
       " 'until',\n",
       " 'you',\n",
       " 'are',\n",
       " 'pliant;\\n\\nAnd',\n",
       " 'then',\n",
       " 'he',\n",
       " 'assigns',\n",
       " 'you',\n",
       " 'to',\n",
       " 'his',\n",
       " 'sacred\\nfire,',\n",
       " 'that',\n",
       " 'you',\n",
       " 'may',\n",
       " 'become',\n",
       " 'sacred',\n",
       " 'bread\\nfor',\n",
       " 'God’s',\n",
       " 'sacred',\n",
       " 'feast.\\n\\n*****\\n\\nAll',\n",
       " 'these',\n",
       " 'things',\n",
       " 'shall',\n",
       " 'love',\n",
       " 'do',\n",
       " 'unto',\n",
       " 'you\\nthat',\n",
       " 'you',\n",
       " 'may',\n",
       " 'know',\n",
       " 'the',\n",
       " 'secrets',\n",
       " 'of',\n",
       " 'your\\nheart,',\n",
       " 'and',\n",
       " 'in',\n",
       " 'that',\n",
       " 'knowledge',\n",
       " 'become',\n",
       " 'a\\nfragment',\n",
       " 'of',\n",
       " 'Life’s',\n",
       " 'heart.\\n\\nBut',\n",
       " 'if',\n",
       " 'in',\n",
       " 'your',\n",
       " 'fear',\n",
       " 'you',\n",
       " 'would',\n",
       " 'seek',\n",
       " 'only\\nlove’s',\n",
       " 'peace',\n",
       " 'and',\n",
       " 'love’s',\n",
       " 'pleasure,\\n\\nThen',\n",
       " 'it',\n",
       " 'is',\n",
       " 'better',\n",
       " 'for',\n",
       " 'you',\n",
       " 'that',\n",
       " 'you\\ncover',\n",
       " '{17}your',\n",
       " 'nakedness',\n",
       " 'and',\n",
       " 'pass',\n",
       " 'out',\n",
       " 'of\\nlove’s',\n",
       " 'threshing-floor,\\n\\nInto',\n",
       " 'the',\n",
       " 'seasonless',\n",
       " 'world',\n",
       " 'where',\n",
       " 'you\\nshall',\n",
       " 'laugh,',\n",
       " 'but',\n",
       " 'not',\n",
       " 'all',\n",
       " 'of',\n",
       " 'your\\nlaughter,',\n",
       " 'and',\n",
       " 'weep,',\n",
       " 'but',\n",
       " 'not',\n",
       " 'all',\n",
       " 'of',\n",
       " 'your\\ntears.\\n\\n*****\\n\\nLove',\n",
       " 'gives',\n",
       " 'naught',\n",
       " 'but',\n",
       " 'itself',\n",
       " 'and',\n",
       " 'takes\\nnaught',\n",
       " 'but',\n",
       " 'from',\n",
       " 'itself.\\n\\nLove',\n",
       " 'possesses',\n",
       " 'not',\n",
       " 'nor',\n",
       " 'would',\n",
       " 'it',\n",
       " 'be\\npossessed;\\n\\nFor',\n",
       " 'love',\n",
       " 'is',\n",
       " 'sufficient',\n",
       " 'unto',\n",
       " 'love.\\n\\nWhen',\n",
       " 'you',\n",
       " 'love',\n",
       " 'you',\n",
       " 'should',\n",
       " 'not',\n",
       " 'say,',\n",
       " '“God\\nis',\n",
       " 'in',\n",
       " 'my',\n",
       " 'heart,”',\n",
       " 'but',\n",
       " 'rather,',\n",
       " '“I',\n",
       " 'am',\n",
       " 'in\\nthe',\n",
       " 'heart',\n",
       " 'of',\n",
       " 'God.”\\n\\nAnd',\n",
       " 'think',\n",
       " 'not',\n",
       " 'you',\n",
       " 'can',\n",
       " 'direct',\n",
       " 'the',\n",
       " 'course\\nof',\n",
       " 'love,',\n",
       " 'for',\n",
       " 'love,',\n",
       " 'if',\n",
       " 'it',\n",
       " 'finds',\n",
       " 'you\\nworthy,',\n",
       " 'directs',\n",
       " 'your',\n",
       " 'course.\\n\\nLove',\n",
       " 'has',\n",
       " 'no',\n",
       " 'other',\n",
       " 'desire',\n",
       " 'but',\n",
       " 'to',\n",
       " 'fulfil\\nitself.\\n\\nBut',\n",
       " 'if',\n",
       " 'you',\n",
       " 'love',\n",
       " 'and',\n",
       " 'must',\n",
       " 'needs',\n",
       " 'have\\ndesires,',\n",
       " 'let',\n",
       " 'these',\n",
       " 'be',\n",
       " 'your',\n",
       " 'desires:\\n\\nTo',\n",
       " 'melt',\n",
       " 'and',\n",
       " 'be',\n",
       " 'like',\n",
       " 'a',\n",
       " 'running',\n",
       " 'brook',\n",
       " 'that\\nsings',\n",
       " 'its',\n",
       " 'melody',\n",
       " 'to',\n",
       " 'the',\n",
       " 'night.',\n",
       " '{18}To\\nknow',\n",
       " 'the',\n",
       " 'pain',\n",
       " 'of',\n",
       " 'too',\n",
       " 'much',\n",
       " 'tenderness.\\n\\nTo',\n",
       " 'be',\n",
       " 'wounded',\n",
       " 'by',\n",
       " 'your',\n",
       " 'own',\n",
       " 'understanding\\nof',\n",
       " 'love;\\n\\nAnd',\n",
       " 'to',\n",
       " 'bleed',\n",
       " 'willingly',\n",
       " 'and',\n",
       " 'joyfully.\\n\\nTo',\n",
       " 'wake',\n",
       " 'at',\n",
       " 'dawn',\n",
       " 'with',\n",
       " 'a',\n",
       " 'winged',\n",
       " 'heart',\n",
       " 'and\\ngive',\n",
       " 'thanks',\n",
       " 'for',\n",
       " 'another',\n",
       " 'day',\n",
       " 'of',\n",
       " 'loving;\\n\\nTo',\n",
       " 'rest',\n",
       " 'at',\n",
       " 'the',\n",
       " 'noon',\n",
       " 'hour',\n",
       " 'and',\n",
       " 'meditate\\nlove’s',\n",
       " 'ecstacy;\\n\\nTo',\n",
       " 'return',\n",
       " 'home',\n",
       " 'at',\n",
       " 'eventide',\n",
       " 'with\\ngratitude;\\n\\nAnd',\n",
       " 'then',\n",
       " 'to',\n",
       " 'sleep',\n",
       " 'with',\n",
       " 'a',\n",
       " 'prayer',\n",
       " 'for\\nthe',\n",
       " 'beloved',\n",
       " 'in',\n",
       " 'your',\n",
       " 'heart',\n",
       " 'and',\n",
       " 'a',\n",
       " 'song',\n",
       " 'of\\npraise',\n",
       " 'upon',\n",
       " 'your',\n",
       " 'lips.\\n\\n[Illustration:',\n",
       " '0029]\\n\\n*****',\n",
       " '*****\\n\\n{19}Then',\n",
       " 'Almitra',\n",
       " 'spoke',\n",
       " 'again',\n",
       " 'and',\n",
       " 'said,\\nAnd',\n",
       " 'what',\n",
       " 'of',\n",
       " '_Marriage_',\n",
       " 'master?\\n\\nAnd',\n",
       " 'he',\n",
       " 'answered',\n",
       " 'saying:\\n\\nYou',\n",
       " 'were',\n",
       " 'born',\n",
       " 'together,',\n",
       " 'and',\n",
       " 'together',\n",
       " 'you\\nshall',\n",
       " 'be',\n",
       " 'forevermore.\\n\\nYou',\n",
       " 'shall',\n",
       " 'be',\n",
       " 'together',\n",
       " 'when',\n",
       " 'the',\n",
       " 'white\\nwings',\n",
       " 'of',\n",
       " 'death',\n",
       " 'scatter',\n",
       " 'your',\n",
       " 'days.\\n\\nAye,',\n",
       " 'you',\n",
       " 'shall',\n",
       " 'be',\n",
       " 'together',\n",
       " 'even',\n",
       " 'in',\n",
       " 'the\\nsilent',\n",
       " 'memory',\n",
       " 'of',\n",
       " 'God.\\n\\nBut',\n",
       " 'let',\n",
       " 'there',\n",
       " 'be',\n",
       " 'spaces',\n",
       " 'in',\n",
       " 'your\\ntogetherness,\\n\\nAnd',\n",
       " 'let',\n",
       " 'the',\n",
       " 'winds',\n",
       " 'of',\n",
       " 'the',\n",
       " 'heavens',\n",
       " 'dance\\nbetween',\n",
       " 'you.\\n\\n*****\\n\\nLove',\n",
       " 'one',\n",
       " 'another,',\n",
       " 'but',\n",
       " 'make',\n",
       " 'not',\n",
       " 'a',\n",
       " 'bond',\n",
       " 'of\\nlove:\\n\\nLet',\n",
       " 'it',\n",
       " 'rather',\n",
       " 'be',\n",
       " 'a',\n",
       " 'moving',\n",
       " 'sea',\n",
       " 'between\\nthe',\n",
       " 'shores',\n",
       " 'of',\n",
       " 'your',\n",
       " 'souls.\\n\\nFill',\n",
       " 'each',\n",
       " 'other’s',\n",
       " 'cup',\n",
       " 'but',\n",
       " 'drink',\n",
       " 'not',\n",
       " 'from\\none',\n",
       " 'cup.\\n\\nGive',\n",
       " 'one',\n",
       " 'another',\n",
       " 'of',\n",
       " 'your',\n",
       " 'bread',\n",
       " 'but',\n",
       " 'eat\\nnot',\n",
       " 'from',\n",
       " 'the',\n",
       " 'same',\n",
       " 'loaf.',\n",
       " '{20}Sing',\n",
       " 'and\\ndance',\n",
       " 'together',\n",
       " 'and',\n",
       " 'be',\n",
       " 'joyous,',\n",
       " 'but',\n",
       " 'let\\neach',\n",
       " 'one',\n",
       " 'of',\n",
       " 'you',\n",
       " 'be',\n",
       " 'alone,\\n\\nEven',\n",
       " 'as',\n",
       " 'the',\n",
       " 'strings',\n",
       " 'of',\n",
       " 'a',\n",
       " 'lute',\n",
       " 'are',\n",
       " 'alone\\nthough',\n",
       " 'they',\n",
       " 'quiver',\n",
       " 'with',\n",
       " 'the',\n",
       " 'same',\n",
       " 'music.\\n\\n*****\\n\\nGive',\n",
       " 'your',\n",
       " 'hearts,',\n",
       " 'but',\n",
       " 'not',\n",
       " 'into',\n",
       " 'each\\nother’s',\n",
       " 'keeping.\\n\\nFor',\n",
       " 'only',\n",
       " 'the',\n",
       " 'hand',\n",
       " 'of',\n",
       " 'Life',\n",
       " 'can',\n",
       " 'contain\\nyour',\n",
       " 'hearts.\\n\\nAnd',\n",
       " 'stand',\n",
       " 'together',\n",
       " 'yet',\n",
       " 'not',\n",
       " 'too',\n",
       " 'near\\ntogether:\\n\\nFor',\n",
       " 'the',\n",
       " 'pillars',\n",
       " 'of',\n",
       " 'the',\n",
       " 'temple',\n",
       " 'stand\\napart,\\n\\nAnd',\n",
       " 'the',\n",
       " 'oak',\n",
       " 'tree',\n",
       " 'and',\n",
       " 'the',\n",
       " 'cypress',\n",
       " 'grow\\nnot',\n",
       " 'in',\n",
       " 'each',\n",
       " 'other’s',\n",
       " 'shadow.\\n\\n[Illustration:',\n",
       " '0032]\\n\\n*****',\n",
       " '*****\\n\\n{21}And',\n",
       " 'a',\n",
       " 'woman',\n",
       " 'who',\n",
       " 'held',\n",
       " 'a',\n",
       " 'babe\\nagainst',\n",
       " ...]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here:\n",
    "prophet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The next step is to create a function that will remove references. \n",
    "\n",
    "We will do this by splitting the string on the `{` character and keeping only the part before this character. Write your function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reference(x):\n",
    "    '''\n",
    "    Input: A string\n",
    "    Output: The string with references removed\n",
    "    \n",
    "    Example:\n",
    "    Input: 'the{7}'\n",
    "    Output: 'the'\n",
    "    '''\n",
    "    # Your code here:\n",
    "    \n",
    "    if x.find(\"{\") != -1:\n",
    "        inicio = x.find(\"{\")\n",
    "        final = x.find(\"}\")+1\n",
    "        return x.replace(string[inicio:final],'')\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our function, use the `map()` function to apply this function to our book, The Prophet. Return the resulting list to a new list called `prophet_reference`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "prophet_reference = list(map(reference, prophet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another thing you may have noticed is that some words contain a line break. Let's write a function to split those words. Our function will return the string split on the character `\\n`. Write your function in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_break(x):\n",
    "    '''\n",
    "    Input: A string\n",
    "    Output: A list of strings split on the line break (\\n) character\n",
    "        \n",
    "    Example:\n",
    "    Input: 'the\\nbeloved'\n",
    "    Output: ['the', 'beloved']\n",
    "    '''\n",
    "    # Your code here:\n",
    "    return x.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the `line_break` function to the `prophet_reference` list. Name the new list `prophet_line`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "prophet_line = list(map(line_break, prophet_reference))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the elements of `prophet_line`, you will see that the function returned lists and not strings. Our list is now a list of lists. Flatten the list using list comprehension. Assign this new list to `prophet_flat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "prophet_flat = [ele for sublist in prophet_line for ele in sublist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2 - Filtering\n",
    "\n",
    "When printing out a few words from the book, we see that there are words that we may not want to keep if we choose to analyze the corpus of text. Below is a list of words that we would like to get rid of. Create a function that will return false if it contains a word from the list of words specified and true otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_filter(x):\n",
    "    '''\n",
    "    Input: A string\n",
    "    Output: true if the word is not in the specified list and false if the word is in the list\n",
    "        \n",
    "    Example:\n",
    "    word list = ['and', 'the']\n",
    "    Input: 'and'\n",
    "    Output: False\n",
    "    \n",
    "    Input: 'John'\n",
    "    Output: True\n",
    "    '''\n",
    "    \n",
    "    word_list = ['and', 'the', 'a', 'an']\n",
    "    # Your code here:\n",
    "    if x not in word_list:\n",
    "        return True\n",
    "    else:\n",
    "        return False   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `filter()` function to filter out the words speficied in the `word_filter()` function. Store the filtered list in the variable `prophet_filter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "prophet_filter = list(filter(word_filter,prophet_flat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge - Part 1\n",
    "\n",
    "Rewrite the `word_filter` function above to not be case sensitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_filter_case(x):\n",
    "   \n",
    "    word_list = ['and', 'the', 'a', 'an']\n",
    "    \n",
    "    # Your code here:\n",
    "    if x.lower() not in word_list:\n",
    "        return True\n",
    "    else:\n",
    "        return False       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 3 - Reducing\n",
    "\n",
    "#### Now that we have significantly cleaned up our text corpus, let's use the `reduce()` function to put the words back together into one long string separated by spaces. \n",
    "\n",
    "We will start by writing a function that takes two strings and concatenates them together with a space between the two strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_space(a, b):\n",
    "    '''\n",
    "    Input:Two strings\n",
    "    Output: A single string separated by a space\n",
    "        \n",
    "    Example:\n",
    "    Input: 'John', 'Smith'\n",
    "    Output: 'John Smith'\n",
    "    '''\n",
    "    \n",
    "    # Your code here:\n",
    "    return a+' '+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function above to reduce the text corpus in the list `prophet_filter` into a single string. Assign this new string to the variable `prophet_string`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "prophet_string =  reduce(concat_space, prophet_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 4 - Applying Functions to DataFrames\n",
    "\n",
    "#### Our next step is to use the apply function to a dataframe and transform all cells.\n",
    "\n",
    "To do this, we will load a dataset below and then write a function that will perform the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code:\n",
    "\n",
    "# The dataset below contains information about pollution from PM2.5 particles in Beijing \n",
    "\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00381/PRSA_data_2010.1.1-2014.12.31.csv\"\n",
    "pm25 = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the data using the `head()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>pm2.5</th>\n",
       "      <th>DEWP</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>PRES</th>\n",
       "      <th>cbwd</th>\n",
       "      <th>Iws</th>\n",
       "      <th>Is</th>\n",
       "      <th>Ir</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-21</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>1021.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-21</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>1020.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>4.92</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-21</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>6.71</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-21</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>9.84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-20</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>1018.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>12.97</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   No  year  month  day  hour  pm2.5  DEWP  TEMP    PRES cbwd    Iws  Is  Ir\n",
       "0   1  2010      1    1     0    NaN   -21 -11.0  1021.0   NW   1.79   0   0\n",
       "1   2  2010      1    1     1    NaN   -21 -12.0  1020.0   NW   4.92   0   0\n",
       "2   3  2010      1    1     2    NaN   -21 -11.0  1019.0   NW   6.71   0   0\n",
       "3   4  2010      1    1     3    NaN   -21 -14.0  1019.0   NW   9.84   0   0\n",
       "4   5  2010      1    1     4    NaN   -20 -12.0  1018.0   NW  12.97   0   0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here:\n",
    "pm25.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create a function that divides a cell by 24 to produce an hourly figure. Write the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hourly(x):\n",
    "    '''\n",
    "    Input: A numerical value\n",
    "    Output: The value divided by 24\n",
    "        \n",
    "    Example:\n",
    "    Input: 48\n",
    "    Output: 2.0\n",
    "    '''\n",
    "    \n",
    "    # Your code here:\n",
    "    return x/24.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply this function to the columns `Iws`, `Is`, and `Ir`. Store this new dataframe in the variable `pm25_hourly`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here:\n",
    "\n",
    "pm25_hourly = pm25[['Iws','Is','Ir']].apply(hourly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our last challenge will be to create an aggregate function and apply it to a select group of columns in our dataframe.\n",
    "\n",
    "Write a function that returns the standard deviation of a column divided by the length of a column minus 1. Since we are using pandas, do not use the `len()` function. One alternative is to use `count()`. Also, use the numpy version of standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sd(x):\n",
    "    '''\n",
    "    Input: A Pandas series of values\n",
    "    Output: the standard deviation divided by the number of elements in the series\n",
    "        \n",
    "    Example:\n",
    "    Input: pd.Series([1,2,3,4])\n",
    "    Output: 0.3726779962\n",
    "    '''\n",
    "    \n",
    "    # Your code here:\n",
    "    return np.std(x)/(x.count() -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Challenge - Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the range (max - min) for the columns: pm2.5,DEWP,TEMP,PRES. Ensure we do not return NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T08:22:31.425139Z",
     "start_time": "2019-07-14T08:22:31.420985Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pm2.5</th>\n",
       "      <th>DEWP</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>PRES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-21</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>1021.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-21</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>1020.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-21</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>1019.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-21</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>1019.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-20</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>1018.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pm2.5  DEWP  TEMP    PRES\n",
       "0    NaN   -21 -11.0  1021.0\n",
       "1    NaN   -21 -12.0  1020.0\n",
       "2    NaN   -21 -11.0  1019.0\n",
       "3    NaN   -21 -14.0  1019.0\n",
       "4    NaN   -20 -12.0  1018.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here:\n",
    "pm25[['pm2.5','DEWP','TEMP','PRES']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pm2.5    994.0\n",
       "DEWP      68.0\n",
       "TEMP      61.0\n",
       "PRES      55.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pm25[['pm2.5','DEWP','TEMP','PRES']].max() - pm25[['pm2.5','DEWP','TEMP','PRES']].min()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
